"""
Script de prueba simple para el scraper de Codelco
"""

import asyncio
import sys
import os

# Agregar el directorio padre al path para importar el scraper
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scrapers.codelco_scraper import CodelcoScraper


async def test_scraper():
    """Prueba bÃ¡sica del scraper"""
    print("ğŸ§ª PRUEBA DEL SCRAPER DE CODELCO")
    print("=" * 50)
    
    scraper = CodelcoScraper()
    
    try:
        print("ğŸ“¡ Testeando conexiÃ³n al sitio web...")
        
        # Crear una sesiÃ³n para hacer la prueba
        import aiohttp
        async with aiohttp.ClientSession() as session:
            # Test 1: Verificar que se puede acceder a la pÃ¡gina principal
            html = await scraper.fetch_page(session, scraper.search_url)
            if html:
                print("âœ… ConexiÃ³n exitosa al sitio web")
                print(f"ğŸ“„ TamaÃ±o de la pÃ¡gina: {len(html)} caracteres")
                
                # Test 2: Extraer empleos bÃ¡sicos
                print("\nğŸ” Extrayendo empleos de la pÃ¡gina...")
                jobs = scraper.extract_job_links_and_basic_info(html)
                
                if jobs:
                    print(f"âœ… Se encontraron {len(jobs)} empleos")
                    
                    # Mostrar los primeros 3 empleos
                    print("\nğŸ“‹ PRIMEROS EMPLEOS ENCONTRADOS:")
                    print("-" * 40)
                    for i, job in enumerate(jobs[:3], 1):
                        print(f"\n{i}. {job['titulo']}")
                        print(f"   ğŸ†” ID: {job['id_proceso']}")
                        print(f"   ğŸ“… Fecha: {job['fecha']}")
                        print(f"   ğŸ“ RegiÃ³n: {job['region']}")
                        print(f"   ğŸ“® CÃ³digo postal: {job['codigo_postal']}")
                        print(f"   ğŸ”— URL: {job['url']}")
                    
                    # Test 3: Probar extracciÃ³n de detalles en el primer empleo
                    if jobs:
                        print(f"\nğŸ” Probando extracciÃ³n de detalles del primer empleo...")
                        first_job = jobs[0]
                        details = await scraper.get_job_details(session, first_job['url'])
                        
                        if details:
                            print("âœ… Detalles extraÃ­dos exitosamente")
                            if 'descripcion' in details:
                                desc_preview = details['descripcion'][:200] + "..." if len(details['descripcion']) > 200 else details['descripcion']
                                print(f"ğŸ“ DescripciÃ³n: {desc_preview}")
                            if 'requisitos' in details:
                                req_preview = details['requisitos'][:200] + "..." if len(details['requisitos']) > 200 else details['requisitos']
                                print(f"ğŸ“‹ Requisitos: {req_preview}")
                            if 'informacion_adicional' in details:
                                print(f"â„¹ï¸  Info adicional: {details['informacion_adicional']}")
                        else:
                            print("âš ï¸  No se pudieron extraer detalles adicionales")
                    
                    print("\nğŸ‰ Â¡Prueba completada exitosamente!")
                    return True
                else:
                    print("âŒ No se pudieron extraer empleos de la pÃ¡gina")
                    return False
            else:
                print("âŒ No se pudo acceder al sitio web")
                return False
                
    except Exception as e:
        print(f"âŒ Error durante la prueba: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = asyncio.run(test_scraper())
    if success:
        print("\nâœ… El scraper estÃ¡ funcionando correctamente")
    else:
        print("\nâŒ El scraper tiene problemas")
