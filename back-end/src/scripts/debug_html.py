"""
Debug script para analizar la estructura HTML del sitio de Codelco
"""

import asyncio
import aiohttp
from bs4 import BeautifulSoup
import re


async def debug_html_structure():
    """Analizar la estructura HTML del sitio"""
    print("üîç ANALIZANDO ESTRUCTURA HTML DE CODELCO")
    print("=" * 60)
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive'
    }
    
    url = "https://empleos.codelco.cl/search/"
    
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(url, headers=headers) as response:
                html = await response.text()
                
                print(f"üìÑ P√°gina obtenida exitosamente ({len(html)} caracteres)")
                
                # Guardar HTML para an√°lisis
                with open("debug_codelco.html", "w", encoding="utf-8") as f:
                    f.write(html)
                print("üíæ HTML guardado en debug_codelco.html")
                
                soup = BeautifulSoup(html, 'html.parser')
                
                # Analizar enlaces con /job/
                job_links = soup.find_all('a', href=re.compile(r'/job/'))
                print(f"\nüîó Encontrados {len(job_links)} enlaces con '/job/'")
                
                if job_links:
                    print("\nüìã PRIMEROS 3 ENLACES:")
                    for i, link in enumerate(job_links[:3], 1):
                        print(f"\n{i}. T√≠tulo: {link.get_text(strip=True)}")
                        print(f"   URL: {link.get('href')}")
                        
                        # Analizar el contenedor padre
                        parent = link.find_parent()
                        level = 1
                        while parent and level <= 10:
                            text = parent.get_text()
                            if 'ID de proceso' in text:
                                print(f"   üì¶ Contenedor encontrado en nivel {level}")
                                print(f"   üìù Contenido del contenedor:")
                                # Extraer l√≠neas relevantes
                                lines = [line.strip() for line in text.split('\n') if line.strip()]
                                for line in lines[:10]:  # Primeras 10 l√≠neas
                                    if any(keyword in line for keyword in ['ID de proceso', 'Fecha', 'Regi√≥n', 'C√≥digo']):
                                        print(f"      {line}")
                                break
                            parent = parent.find_parent()
                            level += 1
                        
                        if level > 10:
                            print("   ‚ùå No se encontr√≥ contenedor con informaci√≥n")
                
                # Buscar patrones espec√≠ficos en el HTML
                print(f"\nüîç BUSCANDO PATRONES EN EL HTML:")
                
                # Buscar IDs de proceso
                id_matches = re.findall(r'ID de proceso (\d+)', html)
                print(f"üìä IDs de proceso encontrados: {len(id_matches)}")
                if id_matches:
                    print(f"   Ejemplos: {id_matches[:5]}")
                
                # Buscar fechas
                fecha_matches = re.findall(r'Fecha (\d{1,2} \w+ \d{4})', html)
                print(f"üìÖ Fechas encontradas: {len(fecha_matches)}")
                if fecha_matches:
                    print(f"   Ejemplos: {fecha_matches[:5]}")
                
                # Buscar regiones
                region_matches = re.findall(r'Regi√≥n ([^C\n]+?)(?=C√≥digo|$)', html)
                print(f"üåç Regiones encontradas: {len(region_matches)}")
                if region_matches:
                    print(f"   Ejemplos: {region_matches[:5]}")
                
                # Analizar la estructura de las secciones de empleos
                print(f"\nüèóÔ∏è  ANALIZANDO ESTRUCTURA DE SECCIONES:")
                
                # Buscar elementos que contengan informaci√≥n de empleos
                job_sections = soup.find_all(string=re.compile(r'ID de proceso'))
                print(f"üìÑ Secciones con 'ID de proceso': {len(job_sections)}")
                
                if job_sections:
                    for i, section in enumerate(job_sections[:3], 1):
                        parent = section.find_parent()
                        if parent:
                            print(f"\n{i}. Secci√≥n #{i}:")
                            print(f"   Tag: {parent.name}")
                            print(f"   Clases: {parent.get('class', [])}")
                            text_content = parent.get_text()
                            # Mostrar l√≠neas relevantes
                            lines = [line.strip() for line in text_content.split('\n') if line.strip()]
                            for line in lines[:8]:
                                if line:
                                    print(f"   {line}")
                
        except Exception as e:
            print(f"‚ùå Error: {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(debug_html_structure())
